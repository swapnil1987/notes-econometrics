# Linear Algebra Foundations {#sec-linear-algebra}

## Introduction

Linear algebra is the backbone of econometrics. This chapter reviews essential concepts with an emphasis on computational implementation.

## Vector Spaces

::: {.callout-note}
## Definition: Vector Space
A **vector space** $V$ over a field $\mathbb{F}$ is a set equipped with addition and scalar multiplication satisfying eight axioms...
:::

### Vectors in Python

```{python}
import numpy as np

# Create vectors
x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

# Vector operations
print(f"x + y = {x + y}")
print(f"3*x = {3*x}")
print(f"Inner product: {np.dot(x, y)}")
print(f"Norm of x: {np.linalg.norm(x)}")
```

## Matrix Operations

### Matrix Multiplication

For matrices $A \in \mathbb{R}^{m \times n}$ and $B \in \mathbb{R}^{n \times p}$, the product $C = AB$ is defined as:

$$
c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj}
$$ {#eq-matrix-mult}

```{python}
# Define matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Matrix multiplication
C = A @ B  # or np.dot(A, B)
print(f"A @ B =\n{C}")

# Element-wise multiplication (Hadamard product)
D = A * B
print(f"\nA * B (element-wise) =\n{D}")
```

## Eigenvalues and Eigenvectors

::: {.callout-important}
## Theorem: Spectral Theorem
A symmetric matrix $A \in \mathbb{R}^{n \times n}$ has $n$ real eigenvalues and can be diagonalized by an orthogonal matrix.
:::

### Computing Eigenvalues

```{python}
# Symmetric matrix
A = np.array([[4, 2], [2, 3]])

# Compute eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)

print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")

# Verify: A*v = lambda*v
v1 = eigenvectors[:, 0]
lambda1 = eigenvalues[0]
print(f"\nVerification: A*v1 = {A @ v1}")
print(f"lambda1*v1 = {lambda1 * v1}")
```

## Matrix Decompositions

### Singular Value Decomposition (SVD)

Every matrix $A \in \mathbb{R}^{m \times n}$ can be decomposed as:

$$
A = U \Sigma V^T
$$

where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal, and $\Sigma$ contains singular values.

```{python}
# SVD decomposition
A = np.array([[1, 2, 3], [4, 5, 6]])
U, s, Vt = np.linalg.svd(A)

print(f"U shape: {U.shape}")
print(f"Singular values: {s}")
print(f"Vt shape: {Vt.shape}")

# Reconstruct A
Sigma = np.zeros_like(A, dtype=float)
np.fill_diagonal(Sigma, s)
A_reconstructed = U @ Sigma @ Vt
print(f"\nReconstructed A:\n{A_reconstructed}")
```

## Applications in Econometrics

### Projection Matrices

In regression analysis, the projection matrix (hat matrix) is:

$$
P = X(X^TX)^{-1}X^T
$$

```{python}
# Generate design matrix
np.random.seed(42)
n, k = 100, 3
X = np.random.randn(n, k)

# Compute projection matrix
P = X @ np.linalg.inv(X.T @ X) @ X.T

# Verify idempotence: P^2 = P
print(f"||P^2 - P|| = {np.linalg.norm(P @ P - P):.2e}")

# Verify symmetry: P = P^T
print(f"||P - P^T|| = {np.linalg.norm(P - P.T):.2e}")
```

## Exercises

::: {.callout-tip}
## Theoretical Exercises

1. Prove that the projection matrix $P$ is idempotent and symmetric.
2. Show that the eigenvalues of a projection matrix are either 0 or 1.
3. Prove the Cauchy-Schwarz inequality: $|x^Ty| \leq \|x\| \|y\|$.

## Computational Exercises

4. Implement the Gram-Schmidt orthogonalization process.
5. Write a function to compute the Moore-Penrose pseudoinverse using SVD.
6. Simulate data from a linear model and compute the projection matrix. Verify that $P\hat{\epsilon} = 0$.
:::

## References

Further reading on computational linear algebra:

- Golub & Van Loan (2013): *Matrix Computations*
- Strang (2016): *Introduction to Linear Algebra*
